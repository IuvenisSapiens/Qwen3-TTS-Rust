"""
[83] Convert Master to GGUF with Monkey Patching
ä½¿ç”¨ Python Monkey Patch æŠ€æœ¯ç»•è¿‡ convert_hf_to_gguf.py çš„å“ˆå¸Œæ ¡éªŒã€‚
å¼ºåˆ¶å°†å½“å‰è¿·ä½  Tokenizer è¯†åˆ«ä¸º "qwen2" ç±»å‹ã€‚
"""

import sys
import os
import shutil
import logging
from unittest.mock import patch
import json
from pathlib import Path

# 1. ç¡®ä¿èƒ½å¯¼å…¥ qwen3_tts_gguf ç›®å½•ä¸‹çš„æ¨¡å—
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
CONVERT_LIB_DIR = os.path.join(PROJECT_ROOT, "qwen3_tts_gguf")

if CONVERT_LIB_DIR not in sys.path:
    # æ’å…¥åˆ°æœ€å‰é¢ä»¥ç¡®ä¿ä¼˜å…ˆåŠ è½½
    sys.path.insert(0, CONVERT_LIB_DIR)

# 2. å¯¼å…¥ç›®æ ‡æ¨¡å—
try:
    import convert_hf_to_gguf

    # get_vocab_base_pre æ˜¯ TextModel çš„æ–¹æ³•
    from convert_hf_to_gguf import TextModel
    import gguf
except ImportError as e:
    print(f"âŒ Error importing convert_hf_to_gguf: {e}")
    sys.exit(1)


# 3. å®šä¹‰è¡¥ä¸å‡½æ•°
def patched_get_vocab_base_pre(self, tokenizer) -> str:
    """
    Monkey Patch æ›¿ä»£å‡½æ•°ã€‚
    ä¸è¿›è¡Œä»»ä½•å“ˆå¸Œè®¡ç®—ï¼Œç›´æ¥è¿”å› 'qwen2'ã€‚
    """
    print(f"ğŸ’‰ [è¡¥ä¸] æ‹¦æˆªåˆ° get_vocab_base_pre è°ƒç”¨ã€‚")
    print(f"ğŸ’‰ [è¡¥ä¸] ç»•è¿‡å“ˆå¸Œæ£€æŸ¥ï¼Œå¼ºåˆ¶è¿”å› 'qwen2'ã€‚")
    return "qwen2"


def patched_load_hparams(dir_model: Path, is_mistral_format: bool):
    """
    Monkey Patch æ›¿ä»£å‡½æ•°ã€‚
    å¼ºåˆ¶ä» config.json åŠ è½½å‚æ•°ï¼Œç»•è¿‡ AutoConfig çš„æ½œåœ¨è¿œç¨‹ä»£ç åŠ è½½å’Œè§£æå¹²æ‰°ã€‚
    """
    print(f"ğŸ’‰ [è¡¥ä¸] æ‹¦æˆªåˆ° load_hparams è°ƒç”¨ã€‚")
    print(f"ğŸ’‰ [è¡¥ä¸] å¼ºåˆ¶ä» {dir_model / 'config.json'} åŠ è½½é…ç½®ã€‚")

    if is_mistral_format:
        with open(dir_model / "params.json", "r", encoding="utf-8") as f:
            config = json.load(f)
        return config

    try:
        with open(dir_model / "config.json", "r", encoding="utf-8") as f:
            config = json.load(f)
    except Exception as e:
        print(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        raise

    if "llm_config" in config:
        config["text_config"] = config["llm_config"]
    if "lm_config" in config:
        config["text_config"] = config["lm_config"]
    if "thinker_config" in config:
        config["text_config"] = config["thinker_config"]["text_config"]
    if "lfm" in config:
        config["text_config"] = config["lfm"]

    return config


from export_config import EXPORT_DIR


# 4. è½¬æ¢ä¸»é€»è¾‘
def main():
    TALKER_MODEL_DIR = os.path.join(EXPORT_DIR, "hf")
    GGUF_OUT = os.path.join(EXPORT_DIR, "qwen3_tts_talker.gguf")

    print(f"--- æ­£åœ¨æ‰§è¡Œ GGUF è½¬æ¢ (Talker) ---")
    print(f"æºç›®å½•: {TALKER_MODEL_DIR}")
    print(f"è¾“å‡ºæ–‡ä»¶: {GGUF_OUT}")

    # [æ­¥éª¤ A] æ£€æŸ¥å¿…è¦æ–‡ä»¶
    required_files = ["model.safetensors", "config.json", "tokenizer.json"]
    for f in required_files:
        if not os.path.exists(os.path.join(TALKER_MODEL_DIR, f)):
            print(f"âŒ é”™è¯¯: ç¼ºå°‘å¿…è¦æ–‡ä»¶ {f}ï¼Œè¯·åˆ¶ä½œæ¨¡å‹åå†è¿è¡Œã€‚")
            sys.exit(1)

    # [æ­¥éª¤ B] åº”ç”¨ Monkey Patch
    print("[1/2] æ­£åœ¨åº”ç”¨è™šæ‹ŸåŠ è½½è¡¥ä¸ (åŠ¨æ€æ˜ å°„æƒé‡é”®)...")

    # Patch 1: TextModel.get_vocab_base_pre (ç»•è¿‡åˆ†è¯å™¨å“ˆå¸Œæ£€æŸ¥)
    TextModel.get_vocab_base_pre = patched_get_vocab_base_pre

    # Patch 2: ModelBase.load_hparams (æ”¯æŒä» config.json åŠ è½½å‚æ•°)
    from convert_hf_to_gguf import ModelBase

    ModelBase.load_hparams = staticmethod(patched_load_hparams)

    # Patch 3: ModelBase.index_tensors (æ ¸å¿ƒï¼šè™šæ‹Ÿæ˜ å°„æƒé‡é”®)
    # GGUF è½¬æ¢å™¨æ¢æµ‹åˆ° qwen2 æ¶æ„æ—¶ï¼Œè¦æ±‚ backbone æƒé‡å¸¦ model. å‰ç¼€
    # æˆ‘ä»¬åœ¨è¯»å–æ—¶åŠ¨æ€åŠ ä¸Šå‰ç¼€ï¼Œä»è€Œé¿å…åœ¨ç£ç›˜ä¸Šå¤åˆ¶å’Œé‡å‘½åæ–‡ä»¶
    original_index_tensors = ModelBase.index_tensors

    def patched_index_tensors(self, *args, **kwargs):
        # è°ƒç”¨åŸå§‹ç´¢å¼•é€»è¾‘è·å–æ‰€æœ‰ Tensor ç”Ÿæˆå™¨
        tensors = original_index_tensors(self, *args, **kwargs)
        new_tensors = {}
        for name, data_gen in tensors.items():
            # lm_head ä¸éœ€è¦å‰ç¼€ï¼›model. å¼€å¤´çš„è¯´æ˜å·²ç»æœ‰å‰ç¼€äº†
            if name.startswith("lm_head") or name.startswith("model."):
                new_tensors[name] = data_gen
            else:
                # ç»™éª¨å¹²ç½‘ç»œåŠ¨æ€åŠ ä¸Š model. å‰ç¼€ (å¦‚ layers.0... -> model.layers.0...)
                new_tensors[f"model.{name}"] = data_gen
        return new_tensors

    ModelBase.index_tensors = patched_index_tensors

    # [æ­¥éª¤ C] è°ƒç”¨è½¬æ¢å™¨ä¸»å‡½æ•°
    print(f"[2/2] æ­£åœ¨è°ƒç”¨ convert_hf_to_gguf.main()...")

    # æ¨¡æ‹Ÿå‘½ä»¤è¡Œå‚æ•°
    sys.argv = [
        "convert_hf_to_gguf.py",
        TALKER_MODEL_DIR,
        "--outfile",
        GGUF_OUT,
        "--outtype",
        "f16",
    ]

    try:
        convert_hf_to_gguf.main()
        print(f"\nâœ… Talker GGUF è½¬æ¢æˆåŠŸ!")
        print(f"è¾“å‡ºè·¯å¾„: {GGUF_OUT}")
    except Exception as e:
        print(f"\nâŒ è½¬æ¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
